{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning using Sentinel-2 Data\n",
    "\n",
    "This example uses training data from the\n",
    "[Coast Train](https://github.com/nick-murray/coastTrain) dataset\n",
    "along with Sentinel-2 data to demonstrate how to use a\n",
    "machine learning classifier, in this case, Random Forest, to\n",
    "assign a class to each pixel.\n",
    "\n",
    "This notebook combines lessons from previous notebooks into\n",
    "a comprehensive worked example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "First we load the required Python libraries and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac_client import Client\n",
    "from dask.distributed import Client as DaskClient\n",
    "from odc.stac import load, configure_rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import folium\n",
    "from ipyleaflet import basemaps\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study site configuration\n",
    "\n",
    "Here we establish the STAC catalog we're using as well as a\n",
    "spatial and temporal extent. This can be anywhere, but this location\n",
    "near Kuching was chosen due to the training data having several\n",
    "classes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC Catalog URL\n",
    "catalog = \"https://earth-search.aws.element84.com/v1\"\n",
    "\n",
    "# Create a STAC Client\n",
    "client = Client.open(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location is east of Zamboanga City, Philippines.\n",
    "ll = (6.87774,122.07482)  # lat/lon order\n",
    "ur = (7.00761,122.28968)\n",
    "bbox = (ll[1], ll[0], ur[1], ur[0])\n",
    "\n",
    "# Four months of data\n",
    "datetime = \"2024-06/2024-09\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure our environment.\n",
    "\n",
    "This cell sets up Dask, which we use for parallel computing, and configures\n",
    "AWS credentials for \"unsigned\" (public) data access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dask cluster to improve data load time.\n",
    "dask_client = DaskClient(n_workers=2, threads_per_worker=16, memory_limit='16GB')\n",
    "\n",
    "# We set up Rasterio to optimise loading data\n",
    "_ = configure_rio(cloud_defaults=True)\n",
    "\n",
    "dask_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "Next up we gather training data. This could be any geospatial point dataset\n",
    "with a column that is numeric, for the class.\n",
    "\n",
    "If you'd like to explore the structure of this data, you can run `gdf.head()`\n",
    "to see the first few rows. The `explore()` function with the `column` argument\n",
    "will show the data on the map, and change the colour based on that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training data\n",
    "data_url = \"https://raw.githubusercontent.com/nick-murray/coastTrain/main/data/coastTrain_v1_0.geojson\"\n",
    "gdf = gpd.read_file(data_url, bbox=bbox)\n",
    "\n",
    "# Alternately, use your updated data if you have it.\n",
    "# gdf = gpd.read_file(\"data/training_revised.geojson\", bbox=bbox)\n",
    "\n",
    "gdf.explore(column=\"Ecosys_Typ\", legend=True, tiles=basemaps.Esri.WorldImagery, style_kwds={\"radius\":5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and load Sentinel-2 data\n",
    "\n",
    "Here we search for Sentinel-2 scenes over our study area and use\n",
    "Dask to lazy-load them. We're only loading the red, green, blue, nir and swir\n",
    "bands, along with the scene classification (scl) band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Sentinel-2 data\n",
    "items = client.search(\n",
    "    collections=[\"sentinel-2-c1-l2a\"],\n",
    "    bbox=bbox,\n",
    "    datetime=datetime,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 90}},  # Remove scenes completely if they have a cloud cover of 90% or more\n",
    ").item_collection()\n",
    "\n",
    "print(f\"Found {len(items)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into an xarray Dataset\n",
    "data = load(\n",
    "    items,\n",
    "    measurements=[\"red\", \"green\", \"blue\", \"nir08\", \"swir16\", \"scl\"],\n",
    "    bbox=bbox,\n",
    "    chunks={\"x\": 2048, \"y\": 2048},\n",
    "    groupby=\"solar_day\",\n",
    "    skip_failures=True,\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Now that we have data, we need to clean it up, masking out clouds\n",
    "and scaling values to between 0-1, which are the valid reflectance\n",
    "values.\n",
    "\n",
    "We add a couple of indices too, which will help the machine learning\n",
    "algorithm.\n",
    "\n",
    "Note that we still have a lazy-loaded array, and haven't transferred\n",
    "any data over the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out clouds and scale values\n",
    "\n",
    "# Apply Sentinel-2 cloud mask\n",
    "# 1: defective, 3: shadow, 8: low confidence cloud, 9: high confidence cloud, 10: thin cirrus\n",
    "mask_flags = [1, 3, 8, 9, 10]\n",
    "\n",
    "cloud_mask = ~data.scl.isin(mask_flags)\n",
    "masked = data.where(cloud_mask)\n",
    "\n",
    "# Apply scaling and clip to valid data, from 0 to 1\n",
    "scaled = (masked.where(masked != 0) * 0.0001).clip(0, 1)\n",
    "\n",
    "# NDVI for vegetation density\n",
    "scaled[\"ndvi\"] = (scaled.nir08 - scaled.red) / (scaled.nir08 + scaled.red)\n",
    "# NDWI for distinguishing between vegetation and water\n",
    "scaled[\"ndwi\"] = (scaled.green - scaled.nir08) / (scaled.green + scaled.nir08)\n",
    "# Modified form of MVI (Baloloy et al. 2020) for distinguishing between mangroves and other vegetation types\n",
    "scaled[\"mvi\"] = (((scaled.nir08 - scaled.green) / (scaled.swir16 - scaled.green)) * 0.1).clip(-1,1)\n",
    "\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise one date, to make sure it looks good.\n",
    "# This example shows empty areas where we've masked out clouds.\n",
    "\n",
    "# This process of loading should take less than a minute\n",
    "scaled.isel(time=0).odc.explore(vmin=0, vmax=0.3, tiles=basemaps.CartoDB.DarkMatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cloud-free composite\n",
    "\n",
    "The final data preparation step involves creating a temporal\n",
    "median of the data bands. Here we use `compute()` to process\n",
    "the data and bring it into memory.\n",
    "\n",
    "We preview the data in the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a median composite, which should get rid of most of the remaining clouds\n",
    "# Note that this will take a few minutes to complete\n",
    "\n",
    "median = scaled.median(\"time\").compute()\n",
    "\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median.odc.explore(vmin=0, vmax=0.3, tiles=basemaps.CartoDB.DarkMatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data array\n",
    "\n",
    "This next step involves extracting observed values from the satellite data\n",
    "and combining them with our point data, resulting in something like this:\n",
    "\n",
    "`class, red, green, blue ...`\n",
    "\n",
    "This structure is then fed into the machine learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First transform the training points to the same CRS as the data\n",
    "training = gdf.to_crs(median.odc.geobox.crs)\n",
    "\n",
    "# Next get the X and Y values out of the point geometries\n",
    "training_da = training.assign(x=training.geometry.x, y=training.geometry.y).to_xarray()\n",
    "\n",
    "# Now we can use the x and y values (lon, lat) to extract values from the median composite\n",
    "training_values = (\n",
    "    median.sel(training_da[[\"x\", \"y\"]], method=\"nearest\").squeeze().compute().to_pandas()\n",
    ")\n",
    "\n",
    "# Join the training data with the extracted values and remove unnecessary columns\n",
    "training_array = pd.concat([training[\"Class\"], training_values], axis=1)\n",
    "training_array = training_array.drop(\n",
    "    columns=[\n",
    "        \"y\",\n",
    "        \"x\",\n",
    "        \"spatial_ref\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Drop rows where there was no data available\n",
    "training_array = training_array.dropna()\n",
    "\n",
    "# Preview our resulting training array\n",
    "training_array.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a classifier and fit a model\n",
    "\n",
    "We pass in simple numpy arrays to the classifier, one has the\n",
    "observations (the values of the red, green, blue and so on)\n",
    "while the other has the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes are the first column\n",
    "classes = np.array(training_array)[:, 0]\n",
    "\n",
    "# The observation data is everything after the first column\n",
    "observations = np.array(training_array)[:, 1:]\n",
    "\n",
    "# Create a model...\n",
    "classifier = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "# ...and fit it to the data\n",
    "model = classifier.fit(observations, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Next we predict. Again, we need a simple numpy array, this time\n",
    "just with the observations. This needs to be in long array where\n",
    "the x dimension is the observation values and the y is each cell\n",
    "in the original raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a stacked array of observations\n",
    "stacked_arrays = median.to_array().stack(dims=[\"y\", \"x\"]).transpose()\n",
    "\n",
    "# Replace any NaN values with 0\n",
    "stacked_arrays = stacked_arrays.fillna(0)\n",
    "\n",
    "# Predict the classes\n",
    "predicted = model.predict(stacked_arrays)\n",
    "\n",
    "# Reshape back to the original 2D array\n",
    "array = predicted.reshape(len(median.y), len(median.x))\n",
    "\n",
    "# Convert to an xarray again, because it's easier to work with\n",
    "predicted_da = xr.DataArray(\n",
    "    array, coords={\"y\": masked.y, \"x\": masked.x}, dims=[\"y\", \"x\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise our results\n",
    "\n",
    "Here we're visualising the results along with the RGB image\n",
    "and the original training data points. We're doing this using\n",
    "a Python library called Folium, which wraps up the Leaflet\n",
    "JavaScript library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all on a single interactive map\n",
    "center = [np.mean([ll[0], ur[0]]), np.mean([ll[1], ur[1]])]\n",
    "m = folium.Map(location=center, zoom_start=11, tiles=basemaps.Esri.WorldImagery)\n",
    "\n",
    "# RGB for the median\n",
    "median.odc.to_rgba(vmin=0, vmax=0.3).odc.add_to(m, name=\"Median Composite\")\n",
    "\n",
    "# Categorical for the predicted classes and for the training data\n",
    "predicted_da.odc.add_to(m, name=\"Predicted\")\n",
    "gdf.explore(m=m, column=\"Class\", name=\"Training Data\", style_kwds={\"radius\":5, \"stroke\":\"white\"})\n",
    "\n",
    "# Layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the RGB median to use as a base for updating training data\n",
    "# median[[\"red\", \"green\", \"blue\"]].odc.to_rgba(vmin=0, vmax=0.3).odc.write_cog(\"median.tif\", overwrite=True)\n",
    "\n",
    "# Export the training data, to update with the more data\n",
    "# gdf[[\"Class\", \"Ecosys_Typ\", \"geometry\"]].to_file(\"data/training_revised.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Do the results make sense?\n",
    "\n",
    "What are some of the limitations of the visualisation?\n",
    "\n",
    "### Next steps and opportunities\n",
    "\n",
    "The obvious next step is to fine tune the data. Perhaps download the points for this\n",
    "region of interest as well as the RGB image and add and remove points until\n",
    "there is a more representative training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
